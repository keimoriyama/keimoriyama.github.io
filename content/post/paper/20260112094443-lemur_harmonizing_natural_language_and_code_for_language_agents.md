+++
title = "Lemur: Harmonizing Natural Language and Code for Language Agents"
author = ["keimoriyama"]
description = "description"
date = 2026-01-12T00:00:00+09:00
tags = ["paper"]
categories = ["paper"]
draft = false
+++

## 📄論文情報 {#論文情報}

-   [Lemur: Harmonizing Natural Language and Code for Language Agents](https://openreview.net/forum?id=hNhwSmtXRh)
-   ICLR Spotlight


## 🔑この論文のキーメッセージ {#この論文のキーメッセージ}

-   事前学習にコーディングタスクを混ぜることで、実用的なLLMを学習することができる。


## 🎓どういう問題に取り組んだのか {#どういう問題に取り組んだのか}

-   一つのLLMで、言語とプログラミングの能力をバランス良く学習すること


## 🧑‍🎓その問題に取り組むことがなぜ重要なのか {#その問題に取り組むことがなぜ重要なのか}

-   LLMの実環境用のインタラクションにおいて、APIを活用する必要がある
    -   チャット機能や推論だけではなく、プログラムを適切に構築する能力も必要になっている
-   ChatGPTなどのClosedなLLMでは、チャット機能とプログラムの両方を実現できている
-   一方で、Llamaなどのモデルは、それぞれのタスク専用のLLMを公開するに留まっている


## 💡問題解決に向けたキーアイデアは何か {#問題解決に向けたキーアイデアは何か}

-   事前学習とインストラクションチューニングに分けて学習する。
    -   事前学習では、コーディングタスクとテキストの割合を10:1にして学習している
        -   学習に使用したプログラムはThe Stackデータセットを活用している
    -   インストラクションチューニングは既存の複数のデータセットを混ぜて使用している
        -   ここのデータはテキストのみ
        -   そこまで工夫している点ではないのかも？


## 👀新たに分かったことは何か {#新たに分かったことは何か}

-   事前学習にコーディングタスクを混ぜて学習し、インストラクションチューニングを適用することで、自然言語を対象とした推論能力を維持しつつ、コーディングタスクにおける性能が向上している
    -   プログラミングだけではなく、APIを使用した推論についても性能が向上している。
    -   事前学習が有効であると言えるのかもしれない。


## ❓疑問点は何か {#疑問点は何か}

-   査読でも指摘されていたけど、事前学習のタスクの割合がどれくらい有効なのか分からない
    -   著者らは継続学習して、調べているらしい
    -   この辺のバランスを良い感じに調整できたら熱い気がする
