+++
title = "From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment"
author = ["keimoriyama"]
description = "description"
date = 2026-02-05T00:00:00+09:00
tags = ["paper"]
categories = ["paper"]
draft = false
+++

## 📄論文情報 {#論文情報}

-   [From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment](https://aclanthology.org/2025.acl-long.946/)
-   ACL 2025 Long


## 🔑この論文のキーメッセージ {#この論文のキーメッセージ}

-   （1, 2文でまとめる）


## 🎓どういう問題に取り組んだのか {#どういう問題に取り組んだのか}

-   Reward Guided Searchは、LLMが生成した複数の文章から報酬モデルの値を用いて文章を選択する手法である
-   この文章の選択に使用する報酬モデルの出力に一貫性を持たせることを目指す


## 🧑‍🎓その問題に取り組むことがなぜ重要なのか {#その問題に取り組むことがなぜ重要なのか}

-   報酬モデルは、文章の全体を評価するように学習される
-   そのため、文章の一部を評価する時に報酬の一貫性が無くなる課題がある
    -   特に、冗長な文章を高く評価する傾向がある
-   この報酬モデルを再学習するには計算コストが多くかかるため、既存のモデルを使用しつつ、報酬の計算を工夫することで、より良い文章の選択が可能になる


## 💡問題解決に向けたキーアイデアは何か {#問題解決に向けたキーアイデアは何か}

-   スコアの計算時にscore consistencyの分析をしている
-   Score Consistencyの定義
    -   文章AとBにおける報酬値r(A)とr(B)がある時、AとBの部分文字列aとbにおける報酬r(a)とr(b)は、以下の関係が成立する
        -   r(A)&gt;r(B) -&gt; r(a) &gt; r(b)
    -   RGSでは、新しいトークンを選択する時に、これまで生成された文章と新しいトークンを繋げた文章の報酬が最大になるように選択するため、重要な性質になる
    -   この時、部分文字列を評価することになるため、文章の部分文字列の評価の一貫性が必要
-   既存の報酬モデルは部分文字列の評価に一貫性が無いため、部分文字列を評価するためのデータセットを構築し、報酬モデルを学習した
-   学習には、報酬モデルの分布の差のエントロピーを重みとして使用している


## 👀新たに分かったことは何か {#新たに分かったことは何か}

-   文章全体を評価するように学習された報酬モデルは、Score Consistencyを満たさない
    -   報酬モデルの学習に使用するデータセットの部分文字列が、良いと評価された文章の方が高くなる割合で評価した
    -   5トークンで、57%くらいであり、50トークンまで増やすと60%まで上がる
    -   一方で、人間との評価の一致度は高い

    -   提案手法により学習した報酬モデルは、5トークンで55%、50トークンで65%まで改善した

-   ベンチマークにおける評価は、報酬の平均値や文章の多様性などで評価
    -   様々なチャンクで文章を区切る方法に提案手法により学習した報酬モデルを使用したっぽい
    -   報酬の値などのスコアを見ると、既存の手法を改善できていると言える


## ❓疑問点は何か {#疑問点は何か}

-   着眼点が良いと思った
-   報酬モデルの学習が必要になっているのはネックになっていないのか気になった。
