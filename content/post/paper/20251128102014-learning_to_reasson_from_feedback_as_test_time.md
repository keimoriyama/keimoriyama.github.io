+++
title = "Learning to Reasson from Feedback as Test-Time"
author = ["keimoriyama"]
description = "description"
date = 2025-11-28T00:00:00+09:00
tags = ["paper"]
categories = ["paper"]
draft = false
+++

## 📄論文情報 {#論文情報}

-   [Learning to Reason from Feedback at Test-Time](https://aclanthology.org/2025.acl-long.262/)
-   ACL 2025 Long


## 🔑この論文のキーメッセージ {#この論文のキーメッセージ}

-   フィードバックを用いてモデルを更新することで、過去の推論結果を活かしつつ推論の性能が向上する


## 🎓どういう問題に取り組んだのか {#どういう問題に取り組んだのか}

-   テスト時におけるフィードバックからLLMを更新する
    -   テスト時において推論を行い、その結果を用いて再度推論するというタスクになる
-   この時に、過去の経験を上手く活用してLLMを更することを目指す


## 🧑‍🎓その問題に取り組むことがなぜ重要なのか {#その問題に取り組むことがなぜ重要なのか}

-   従来の手法では、Sequential RevisionとParallel samplingがある
    -   Sequential Revisionは、過去のトライアル結果をプロンプトに含める方法
    -   Parallel Samplingは過去の結果に関わらず、何度か予測する方法になる
-   Sequential Revisionはコンテキスト長が長くなりやすいため、計算コストが高くなりやすく位置バイアスの影響もある
-   Parallel samplingは効率的であるが、過去のエラーを考慮しない課題がある


## 💡問題解決に向けたキーアイデアは何か {#問題解決に向けたキーアイデアは何か}

-   過去のトライアルよりも、モデルの重みに重点を置いた手法を提案いている
    -   損失関数と効率的なOptimizerを提案している
-   LLMは問題に対する解答をすると、検証モデルが正解かどうかを判定する
    -   不正解である場合、検証モデルは不正解であるという固定の文章を生成する
    -   追加のフィードバックとしてLLMが文章生成する
    -   これらの二つのフィードバックに対してクロスエントロピーが最小になるように学習を進める
    -   モデルのパラメータ内に過去の経験が保存されるという話
        -   Optimizerについてはよく分からなかった
    -   PEFTを参考にしたみたい。。。？


## 👀新たに分かったことは何か {#新たに分かったことは何か}

-   Parallel Samplingでは20GPU/hだったのに対して、提案手法では4GPU/hに改善された
-   トライアルの回数毎に比較すると、提案手法は回数が増える程性能が良くなっている
    -   手法によっては、低下しているものもある
-   Optimizerの比較では、LoRAと比較して少ないパラメータで良い性能になっている


## ❓疑問点は何か {#疑問点は何か}

-   Sequential Samplingと提案手法の計算コストが違いすぎないか
-   Optmizerの立ち位置が分からない
    -   これ別の手法ではない？
