+++
title = "Personalized LLM Decoding via Contrasting Personal Preference"
author = ["keimoriyama"]
description = "description"
date = 2025-11-25T00:00:00+09:00
tags = ["paper"]
categories = ["paper"]
draft = false
+++

## 📄論文情報 {#論文情報}

-   [Personalized LLM Decoding via Contrasting Personal Preference](https://aclanthology.org/2025.emnlp-main.1723/)
-


## 🔑この論文のキーメッセージ {#この論文のキーメッセージ}

-   DPOを基にした報酬を活用して文章のデコーディングや負例の選択をすることは、パーソナライズにおいて有効である


## 🎓どういう問題に取り組んだのか {#どういう問題に取り組んだのか}

-   LLMが文章を生成する時に、ユーザーの意図を推定しながら文章を生成するようにする


## 🧑‍🎓その問題に取り組むことがなぜ重要なのか {#その問題に取り組むことがなぜ重要なのか}

-   ユーザーの意図に沿う応答を生成することはLLMの実用上重要である
-   現状は、プロンプトベースの方法とLoRAなどモデルのパラメータを更新する方法の二種類がある
-   プロンプトベースの手法では、ユーザーのデータから学習することが無いため効果が限定的である課題がある
-   パラメータを更新する手法では、破滅的忘却や計算コストの面から課題がある


## 💡問題解決に向けたキーアイデアは何か {#問題解決に向けたキーアイデアは何か}

-   基本的にLoRAを想定した手法になっている
-   文章のデコーディングには、報酬ベースの手法を使用している
    -   閾値より大きな確率のトークン集合を得る
    -   基モデルとLoRAを適用したモデルがそのトークンを生成する確率の比を報酬とする
    -   この報酬が最大になるトークンを選択してデコーディングする
-   モデルの学習には、DPOを使用している
    -   データセットの構築のためには、LLMが生成したいくつかの例の中から上記の報酬が最も小さいものを負例としている


## 👀新たに分かったことは何か {#新たに分かったことは何か}

-   プロンプトベースの手法は、性能向上が限定的であること
    -   ベースモデルよりも悪くなることがある
    -   特に長文において性能が低下することが確認できた
-   提案手法は、学習ベースの手法よりも良いモデルが学習できていた
    -   報酬ベースのデコーダとDPOの効果は同程度であった


## ❓疑問点は何か {#疑問点は何か}
