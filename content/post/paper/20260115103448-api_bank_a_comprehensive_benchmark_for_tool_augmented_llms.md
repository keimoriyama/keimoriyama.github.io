+++
title = "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs"
author = ["keimoriyama"]
description = "description"
date = 2026-01-15T00:00:00+09:00
tags = ["paper"]
categories = ["paper"]
draft = false
+++

## 📄論文情報 {#論文情報}

-   [API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs](https://aclanthology.org/2023.emnlp-main.187/)
-   EMNLP 2023 main


## 🔑この論文のキーメッセージ {#この論文のキーメッセージ}

-   Tool Callingタスクのデータの構築において、多様なドメインを含めることが重要である。


## 🎓どういう問題に取り組んだのか {#どういう問題に取り組んだのか}

-   LLMが外部ツールを使用する能力を評価するためのベンチマークを構築した


## 🧑‍🎓その問題に取り組むことがなぜ重要なのか {#その問題に取り組むことがなぜ重要なのか}

-   LLMの性能は学習データに依存するため、最新の情報を応答に反映することができない
-   外部ツールを使用することで、最新の情報に対応することができるが、LLMの外部ツールの性能評価はされていない


## 💡問題解決に向けたキーアイデアは何か {#問題解決に向けたキーアイデアは何か}

-   LLMのAPI呼び出し性能を評価するためのベンチマークデータセットと学習データを構築した
-   ベンチマークデータセットの構築
    -   LLMの能力を評価する上で、APIの呼び出し回数と呼び出すことのできるAPIの数を基準にタスクを構築
    -   タスクの分類は以下の三種類
        -   Call : 一回以上のAPI呼び出しで、APIの数が少ない
        -   Retrieve+Call : 一回のAPI呼び出しで、APIの数が多い。LLMには使用できるAPIが与えられない。
        -   Plan+Retrieve+Call : 複数回の呼び出しで、APIの数が多い。LLMには使用できるAPIが与えられない
    -   ベンチマークに使用されるAPIは、実際に実装している(おそらく架空のAPI)
    -   アノテーションは人手で行うようにしている
    -   評価指標は、LLMが作成したクエリの正解率とAPIの応答を基に生成した文章のROUGE-Lスコアを使用している
-   学習データセットの構築
    -   データセットはLLMを用いて作成された合成データセットを用いる
    -   生成は五つのLLMが独立してデータを生成する
        1.  ヘルスケアなどのデータのドメインを指定する
        2.  ドメインを基にAPIを合成する、合成時には実データを例として与えている
        3.  合成されたAPIをランダムサンプリングし、クエリを作成する
        4.  APIのレスポンスを生成する
        5.  データセットに沿う内容になっているか、評価し、フィルターする


## 👀新たに分かったことは何か {#新たに分かったことは何か}

-   Lynxというモデルを提案手法により作成されたデータセットを用いて評価した
-   学習することで、LLMの性能が向上することが分かった
    -   同じようなデータになっているなら、当然な気がする
    -   エラーの傾向として、学習前はAPIの呼び出しが無いケースが多いが、学習後はAPIの関数名の間違えているケースに変化した
-   ベンチマークについては、GPTなどのモデルと比較すると、Callが一番簡単で、Plan+Retrieve+Callが難しい傾向がある
-   ToolAlpacaと比較すると、少ないデータで同等の性能が得られた
    -   高品質なデータであると言えるのか？評価データによって結果が変わりそう


## ❓疑問点は何か {#疑問点は何か}

-   ベンチマークと学習データを同じような方針で作成したら、評価結果が良くなるのは当たり前ではと思った
-   手法自体は参考になりそう
